{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## AlphaMEV\n",
    "### Goal of AlphaMEV is to automate/generalise most common MEV extraction on EVM comptiable block-chains. You can read more details about the way this project initially started on our twitter.\n",
    "### While true North Star of this project is very far away and it's only the beggining, we've decided to host an ML competition to gather ideas from community and compare it against benchmarks.\n",
    "\n",
    "## Competition Information\n",
    "### Goal of this competition is to predict back-runable transactions and cumulative miner's profit that this transaction would generate. There are many examples of transactions which open MEV opportunities after them:\n",
    "1) Oracle updates allow to perform liquidations.\n",
    "2) Large AMM swaps allow to perform cross-DEX arbitrage.\n",
    "3) Accepted govenance proposals which change pool parameters.\n",
    "And many others.\n",
    "\n",
    "## Each row of the training dataset contains following columns:\n",
    "1) txHash - transaction hash on Ethereum blockchain\n",
    "2) txData - dictionary representing all basic transaction information\n",
    "3) txTrace - Geth-style transaction trace\n",
    "4) Label0 - Binary label whether this transaction is back-runable.\n",
    "5) Label1 - Total amount of ETH sent to miners as bribes via MEV-bundles due to this transaction.\n",
    "\n",
    "## You can find link to the dataset below, it's a zip archive containing 2 files: \"train.csv\" and \"test.csv\".\n",
    "## For each row in \"test.csv\" you're expected to generate two predictions separated by comma:\n",
    "1) P[Label0 == 1]\n",
    "2) E[Label1 | Label0 == 1]\n",
    "## You can also find most basic solution in Python which generates required predictions in correct format using the link below.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import csv\n",
    " \n",
    "# Solution is kept trivial and highly inefficient on purpose as it's provided\n",
    "# purely as an example which should be straightforward to beat by anyone\n",
    "def convert_dataset(dataset):\n",
    "  examples = []\n",
    "  for blob in dataset['txData']:\n",
    "    txData = ast.literal_eval(blob)\n",
    "    examples.append([\n",
    "      int(txData['from'], 0) % (2 ** 30),\n",
    "      (int(txData['to'], 0) if txData['to'] is not None else 0) % (2 ** 30),\n",
    "      int(txData['gas'], 0),\n",
    "      int(txData['gasPrice'], 0),\n",
    "      (int(txData['input'][:10], 0) if txData['input'] != '0x' else 0) % (2 ** 30),\n",
    "      int(txData['nonce'], 0),\n",
    "    ])\n",
    "  return np.array(examples)\n",
    " \n",
    "train = pandas.read_csv('train.csv')\n",
    "test = pandas.read_csv('test.csv')\n",
    "testFeatures = convert_dataset(test)\n",
    " \n",
    "binaryModel = xgb.XGBClassifier(n_estimators=50)\n",
    "binaryModel.fit(convert_dataset(train), train['Label0'])\n",
    "binaryPredictions = binaryModel.predict_proba(testFeatures)[:, 1]\n",
    " \n",
    "regressionModel = xgb.XGBRegressor(n_estimators=50)\n",
    "regressionModel.fit(\n",
    "  convert_dataset(train[train['Label0'] == True]),\n",
    "  train[train['Label0'] == True]['Label1']\n",
    ")\n",
    "regressionPredictions = regressionModel.predict(testFeatures)\n",
    " \n",
    "submission = csv.writer(open('submission.csv', 'w', encoding='UTF8'))\n",
    "for x, y in zip(binaryPredictions, regressionPredictions):\n",
    "  submission.writerow([x, y])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/data/Projects/MachineLearnECT/env/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[18:37:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "2dd582f4d494157b7b25c87d8749050f5acfadec8e6aca5fd38f0bbbb455f280"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}