{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## AlphaMEV\n",
    "### Goal of AlphaMEV is to automate/generalise most common MEV extraction on EVM comptiable block-chains. You can read more details about the way this project initially started on our twitter.\n",
    "### While true North Star of this project is very far away and it's only the beggining, we've decided to host an ML competition to gather ideas from community and compare it against benchmarks.\n",
    "\n",
    "## Competition Information\n",
    "### Goal of this competition is to predict back-runable transactions and cumulative miner's profit that this transaction would generate. There are many examples of transactions which open MEV opportunities after them:\n",
    "1) Oracle updates allow to perform liquidations.\n",
    "2) Large AMM swaps allow to perform cross-DEX arbitrage.\n",
    "3) Accepted govenance proposals which change pool parameters.\n",
    "And many others.\n",
    "\n",
    "## Each row of the training dataset contains following columns:\n",
    "1) txHash - transaction hash on Ethereum blockchain\n",
    "2) txData - dictionary representing all basic transaction information\n",
    "3) txTrace - Geth-style transaction trace\n",
    "4) Label0 - Binary label whether this transaction is back-runable.\n",
    "5) Label1 - Total amount of ETH sent to miners as bribes via MEV-bundles due to this transaction.\n",
    "\n",
    "## You can find link to the dataset below, it's a zip archive containing 2 files: \"train.csv\" and \"test.csv\".\n",
    "## For each row in \"test.csv\" you're expected to generate two predictions separated by comma:\n",
    "1) P[Label0 == 1]\n",
    "2) E[Label1 | Label0 == 1]\n",
    "## You can also find most basic solution in Python which generates required predictions in correct format using the link below.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Solution is kept trivial and highly inefficient on purpose as it's provided\n",
    "# purely as an example which should be straightforward to beat by anyone\n",
    "def convert_dataset(dataset):\n",
    "  examples = []\n",
    "  # dat = dataset['txData'].map(ast.literal_eval)\n",
    "  # trc = dataset['txTrace'].map(ast.literal_eval)  \n",
    "\n",
    "  # txTrace = pd.json_normalize(trc)\n",
    "  # print(txTrace.columns)\n",
    "  # err_le = LabelEncoder()\n",
    "  # err_label = err_le.fit_transform(txTrace['error'])\n",
    "  # err_ohe = OneHotEncoder()\n",
    "  # dataset['errorLabel'] = err_label\n",
    "\n",
    "  # err_feature_arr = err_ohe.fit_transform(dataset[['errorLabel']]).toarray()\n",
    "  # # print((err_feature_arr))\n",
    "  # err_feature_label = list(err_le.classes_)\n",
    "  # # print(err_feature_label)\n",
    "  # err_feature = pd.DataFrame(err_feature_arr.astype(int).astype(str))\n",
    "  # err_feature['error'] = err_feature.stack().groupby(level=0).apply(''.join)\n",
    "  \n",
    "\n",
    "  for dat, trc in zip(dataset['txData'], dataset['txTrace']):  \n",
    "    data = ast.literal_eval(dat)\n",
    "    trace = ast.literal_eval(trc)\n",
    "    examples.append([\n",
    "      int(data.get('blockNumber'), 0),\n",
    "      int(data.get('from'), 0) % (2 ** 30),\n",
    "      (int(data.get('to'), 0) if data.get('to') is not None else 0) % (2 ** 30),\n",
    "      int(data.get('gas'), 0),\n",
    "      ((int(trace.get('gasUsed'), 0) if trace.get('gasUsed') is not None else 0) *int(data.get('gasPrice'), 0)),\n",
    "      (int(data.get('input')[:10], 0) if data.get('input') != '0x' else 0) % (2 ** 30),\n",
    "      int(data.get('nonce'), 0),\n",
    "      (1 if  trace.get('error') is not None else 0)\n",
    "    ])\n",
    "  return np.array(examples)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "testFeatures = convert_dataset(test)\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(convert_dataset(train), train['Label0'], test_size=0.2)\n",
    "\n",
    "\n",
    "print('Training data shape: {}'.format(X_train.shape))\n",
    "print('Test data shape: {}'.format(X_test.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data shape: (656347, 8)\n",
      "Test data shape: (164087, 8)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "binaryModel = xgb.XGBClassifier(n_estimators=50)\n",
    "binaryModel.fit(X_train, y_train)\n",
    "pred = binaryModel.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(pred.shape)\n",
    "binaryPredictions = binaryModel.predict_proba(testFeatures)[:, 1]\n",
    " \n",
    "regressionModel = xgb.XGBRegressor(n_estimators=50)\n",
    "regressionModel.fit(\n",
    "  convert_dataset(train[train['Label0'] == True]),\n",
    "  train[train['Label0'] == True]['Label1']\n",
    ")\n",
    "regressionPredictions = regressionModel.predict(testFeatures)\n",
    " \n",
    "# submission = csv.writer(open('submission.csv', 'w', encoding='UTF8'))\n",
    "# for x, y in zip(binaryPredictions, regressionPredictions):\n",
    "#   submission.writerow([x, y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import pylab as plt\n",
    "def plot_learning_curve(history):\n",
    "    \"\"\" Function that accepts the result from a training run and generates loss curves. \"\"\"\n",
    "    plt.plot(history[\"loss\"], label=\"training loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# We extract the number of classes and the input shape from the data\n",
    "num_classes = len(np.unique(y_train))\n",
    "input_shape = X_train.shape[1:]\n",
    "print(input_shape)\n",
    "print('Training Features:\\n   Shape: {}\\n   Type: {}\\n'.format(X_train.shape, X_train.dtype))\n",
    "print('Training Targets:\\n   Shape: {}\\n   Type: {}'.format(y_train.shape, y_train.dtype))\n",
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# ADD LAYERS HERE\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, input_shape=input_shape, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "model.build(input_shape)\n",
    "# This will print an overview of the network architecture\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.1), \n",
    "              loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "result = model.fit(X_train.astype('float32'),\n",
    "          y_train.astype('float32'),\n",
    "          batch_size=50,\n",
    "          epochs=20)\n",
    "\n",
    "test_accuracy = model.evaluate(X_test.astype('float32'), y_test.astype('float32'), verbose=0)\n",
    "\n",
    "print('Test accuracy: {:.04}'.format(test_accuracy))\n",
    "# plot_learning_curve(result.history)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8,)\n",
      "Training Features:\n",
      "   Shape: (656347, 8)\n",
      "   Type: object\n",
      "\n",
      "Training Targets:\n",
      "   Shape: (656347,)\n",
      "   Type: bool\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 2)                 4         \n",
      "=================================================================\n",
      "Total params: 1,287\n",
      "Trainable params: 1,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "13127/13127 [==============================] - 13s 996us/step - loss: 0.5000\n",
      "Epoch 2/20\n",
      "13127/13127 [==============================] - 12s 894us/step - loss: 0.5000\n",
      "Epoch 3/20\n",
      "13127/13127 [==============================] - 13s 1ms/step - loss: 0.5000\n",
      "Epoch 4/20\n",
      "13127/13127 [==============================] - 14s 1ms/step - loss: 0.5000\n",
      "Epoch 5/20\n",
      "13127/13127 [==============================] - 14s 1ms/step - loss: 0.5000\n",
      "Epoch 6/20\n",
      "13127/13127 [==============================] - 11s 836us/step - loss: 0.5000\n",
      "Epoch 7/20\n",
      "13127/13127 [==============================] - 11s 841us/step - loss: 0.5000\n",
      "Epoch 8/20\n",
      "13127/13127 [==============================] - 11s 857us/step - loss: 0.5000\n",
      "Epoch 9/20\n",
      "13127/13127 [==============================] - 11s 863us/step - loss: 0.5000\n",
      "Epoch 10/20\n",
      "13127/13127 [==============================] - 11s 854us/step - loss: 0.5000\n",
      "Epoch 11/20\n",
      "13127/13127 [==============================] - 11s 855us/step - loss: 0.5000\n",
      "Epoch 12/20\n",
      "13127/13127 [==============================] - 11s 860us/step - loss: 0.5000\n",
      "Epoch 13/20\n",
      "13127/13127 [==============================] - 11s 851us/step - loss: 0.5000\n",
      "Epoch 14/20\n",
      "13127/13127 [==============================] - 11s 863us/step - loss: 0.5000\n",
      "Epoch 15/20\n",
      "13127/13127 [==============================] - 12s 936us/step - loss: 0.5000\n",
      "Epoch 16/20\n",
      "13127/13127 [==============================] - 12s 907us/step - loss: 0.5000\n",
      "Epoch 17/20\n",
      "13127/13127 [==============================] - 12s 909us/step - loss: 0.5000\n",
      "Epoch 18/20\n",
      "13127/13127 [==============================] - 13s 1ms/step - loss: 0.5000\n",
      "Epoch 19/20\n",
      "13127/13127 [==============================] - 11s 875us/step - loss: 0.5000\n",
      "Epoch 20/20\n",
      "13127/13127 [==============================] - 12s 877us/step - loss: 0.5000\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "2dd582f4d494157b7b25c87d8749050f5acfadec8e6aca5fd38f0bbbb455f280"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}